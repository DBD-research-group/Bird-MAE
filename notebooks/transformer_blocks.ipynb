{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import Block\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "\n",
    "blocks = nn.ModuleList([\n",
    "            Block(768, 12, 4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "            for i in range(12)])\n",
    "\n",
    "model_test = vit_b_16(weights=None)\n",
    "encoder = model_test.encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision_out tensor([[[ 0.8615,  0.3818, -1.3666,  ...,  1.0656,  1.2554,  0.8793],\n",
      "         [ 2.2541, -0.1871, -1.9430,  ..., -1.9097, -1.5469,  0.3371],\n",
      "         [-0.3748, -2.4048, -0.5863,  ...,  0.8017, -0.6865, -0.0135],\n",
      "         ...,\n",
      "         [ 0.0369, -0.8304, -1.5174,  ..., -0.0620,  0.3432,  0.1470],\n",
      "         [-1.1165,  0.2847, -0.8672,  ..., -1.3847, -0.5802,  0.1011],\n",
      "         [ 1.1991, -0.1397,  0.1841,  ...,  0.6027, -1.2680,  0.8127]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "timm_out tensor([[[ 1.7336,  0.6277, -1.6112,  ..., -0.8935,  1.9955,  0.2163],\n",
      "         [ 3.3105,  0.0748, -2.6195,  ..., -1.9305, -0.7123,  2.7544],\n",
      "         [ 1.1776, -0.2059, -1.2402,  ..., -0.6000, -0.7146,  0.2376],\n",
      "         ...,\n",
      "         [-1.5930, -1.3745, -1.0011,  ..., -1.2161,  1.0292, -0.7924],\n",
      "         [ 1.3192,  0.6448, -1.0268,  ...,  0.8406, -2.2569, -1.8102],\n",
      "         [ 2.3371,  0.8293,  0.0860,  ...,  0.3968,  0.8248, -0.2288]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_patches = 196 # 14x14 patches in 256x256 image\n",
    "x = torch.randn(1, 196+1, 768) #+1 fpr cls_token\n",
    "\n",
    "# torch encoder\n",
    "torchvision_out = encoder(x)\n",
    "print(\"torchvision_out\", torchvision_out)\n",
    "\n",
    "\n",
    "for block in blocks: \n",
    "    x = block(x)\n",
    "print(\"timm_out\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   pred_reshaped = pred.reshape(batch_size, 16, 16, 32, 8)\n",
    "   full_spec = pred_reshaped.permute(0, 1, 3, 2, 4).reshape(batch_size, 16*32, 16*8)\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   plt.figure(figsize=(10, 5))\n",
    "   plt.imshow(full_spec[0].cpu().detach().numpy(), aspect='auto', origin='lower')\n",
    "   plt.colorbar()\n",
    "   plt.title('Reconstructed Spectrogram')\n",
    "   plt.xlabel('Time')\n",
    "   plt.ylabel('Frequency')\n",
    "   plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdmae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
