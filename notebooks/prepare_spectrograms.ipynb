{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram Mapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.compliance.kaldi import fbank\n",
    "import torch \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def prepare_dataset(batch, label_key=\"human_labels\", sample_frequency=32_000):\n",
    "    data = [torch.from_numpy(b[\"array\"]) for b in batch[\"audio\"]]\n",
    "\n",
    "    imgs = []\n",
    "    for d in data: \n",
    "        img = fbank(\n",
    "            d.unsqueeze(0),\n",
    "            htk_compat=True,\n",
    "            sample_frequency=sample_frequency,\n",
    "            use_energy=False,\n",
    "            window_type='hanning',\n",
    "            num_mel_bins=128,\n",
    "            dither=0.0,\n",
    "            frame_shift=10\n",
    "        )\n",
    "        imgs.append(img.T) # after .T: width, height\n",
    "    imgs = [Image.fromarray(img.numpy()) for img in imgs]\n",
    "    \n",
    "    batch['input_values'] = imgs\n",
    "    batch[\"label\"] = batch[label_key]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def get_prepare_dataset_fn(label_key=\"human_labels\", sample_frequency=32_000):\n",
    "    return lambda batch: prepare_dataset(batch, label_key, sample_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioSet - Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a7eb9a8d9a40ad9574fc3da9a96430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb7f653a9d143acb11a076ff0903f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Audio, load_dataset, Sequence, ClassLabel\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"agkphysics/AudioSet\", \n",
    "    cache_dir=\"/home/lrauch/projects/birdMAE/data/audioset_balanced\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=32_000))\n",
    "\n",
    "def _one_hot_encode(batch): # mapping does not work here for some reason \n",
    "    label_list = [y for y in batch[\"human_labels\"]]\n",
    "    \n",
    "    class_one_hot_matrix = np.zeros((len(label_list), 527), dtype=np.float32)\n",
    "    \n",
    "    for class_idx, indices in enumerate(label_list):\n",
    "        class_one_hot_matrix[class_idx, indices] = 1.0\n",
    "    \n",
    "    return {\"human_labels\": class_one_hot_matrix}\n",
    "\n",
    "with open(\"/home/lrauch/projects/birdMAE/data/audioset_ontology_custom527.json\", \"r\") as f:\n",
    "    ontology = json.load(f)\n",
    "\n",
    "num_classes = len(ontology)\n",
    "label_names = list(ontology.keys())\n",
    "class_label = Sequence(ClassLabel(num_classes=num_classes, names=label_names))\n",
    "dataset = dataset.cast_column(\"human_labels\", class_label)\n",
    "dataset = dataset.map(_one_hot_encode, batched=True, batch_size=1000, load_from_cache_file=True)\n",
    "\n",
    "rows_to_remove = [15_759,17_532] #corrupted\n",
    "all_indices = list(range(len(dataset[\"train\"])))\n",
    "indices_to_keep = [i for i in all_indices if i not in rows_to_remove]\n",
    "dataset[\"train\"] = dataset[\"train\"].select(indices_to_keep)\n",
    "\n",
    "rows_to_remove = [6_182] #corrupted\n",
    "all_indices = list(range(len(dataset[\"test\"])))\n",
    "indices_to_keep = [i for i in all_indices if i not in rows_to_remove]\n",
    "dataset[\"test\"] = dataset[\"test\"].select(indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 55.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "for i in tqdm(range(500)):\n",
    "    dataset[\"train\"][i][\"audio\"][\"array\"] # 56 it/s, 8 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create spectrograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31eabb116ece4be29abca29d134cefc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/18683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2861b64306d946959ca33d4e6bd78044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/17141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5db41476ec4515ac1c899eaf536dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/20 shards):   0%|          | 0/18683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773e1c65cbde46e79ed72b512d164ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/17141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset= dataset.map(\n",
    "    get_prepare_dataset_fn(\n",
    "        label_key=\"human_labels\",\n",
    "        sample_frequency=32_000\n",
    "    ),\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=5)\n",
    "\n",
    "dataset.save_to_disk(\"../data/audioset_balanced/audioset_balanced_prepared_32\") \n",
    "# size: 18GB, if audiofiles: 47GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2719.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "for i in tqdm(range(500)):\n",
    "    np.array(dataset[\"train\"][i][\"input_values\"]) # 2719 it/s, 0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654a3ffdaf9146a49b0560574a576274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14cdbf937e249c98d911d9e74189912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../data/audioset_balanced/audioset_balanced_prepared_32\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdmae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
