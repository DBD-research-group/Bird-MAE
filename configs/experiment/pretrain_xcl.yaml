# @package _global_
defaults:
  - override /callbacks: default_ssl
  - override /data/dataset: XCL
  - override /data/loaders: audioset_balanced
  - override /data/transform: pretrain_melbank_birdset
  - override /logger: mlflow
  - override /module/network: amae_vitbase_16
  - override /module/optimizer: adamw
  - override /module/scheduler: cosine
  - override /module/loss: mean_squared_error # remove this or improve
  - override /paths: cluster
  #- override /trainer: single_gpu
  - override /trainer: multi_gpu


seed: 42
start_time: ${now:%Y-%m-%d_%H%M%S}
task_name: "pretrain_xcl" # improve this

data:
  loaders:
    train: 
      num_workers: 24
      batch_size: 128
      shuffle: true
      pin_memory: false
      drop_last: true

logger:
  experiment_name: ${task_name}

trainer:
  max_epochs: 80
  limit_val_batches: 0.0
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  strategy:
    class_path: pytorch_lightning.strategies.DDPStrategy
    init_args:
      find_unused_parameters: true

module:
  optimizer:
    target:
      lr: 2e-4
    extras:
      layer_decay: 0.0
  network:
    sampling_rate: 32_000 

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/callback_checkpoints
    filename: ${module.network.name}_${data.dataset.name}_{epoch:02d}
    save_last: true
    save_weights_only: false
    every_n_epochs: 5
    save_top_k: -1
    save_on_train_epoch_end: true



