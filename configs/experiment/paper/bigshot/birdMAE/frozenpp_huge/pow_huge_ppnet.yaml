# @package _global_
defaults:
  - override /callbacks: default_ssl
  - override /data/dataset: POW
  - override /data/loaders: esc50
  - override /data/transform: melbank_birdset
  - override /logger: mlflow
  - override /module/network: vit_huge_16.yaml
  - override /module/optimizer: adamw
  - override /module/scheduler: cosine
  - override /module/loss: asymmetric_loss.yaml
  - override /module/metric: birdset_collection
  - override /paths: cluster
  - override /trainer: single_gpu

seed: 1
start_time: ${now:%Y-%m-%d_%H%M%S}
task_name: "finetune_pow"

train: true
test: true

logger:
  experiment_name: full_pow
  run_name: birdMAE_pow_huge_ppnet_frozen_${seed}_${start_time}

data:
  loaders:
    train: 
      num_workers: 16
      batch_size: 64
      #batch_size: 20
      shuffle: true
      drop_last: true
  dataset:
    save_to_disk: /scratch/birdset/POW/POW_64shot_1
    
trainer:
  max_epochs: 30
  limit_val_batches: 1.0
  check_val_every_n_epoch: 2
  #gradient_clip_val: 0.1
  gradient_clip_val: 2
  precision: 16-mixed
  num_sanity_val_steps: 0


module:
  optimizer:
    target:
      #lr: 5e-5
      lr: 3e-4
      #lr: 5e-4
      #lr: 1e-4
      #weight_decay: 3e-2
      weight_decay: 3e-4
      
    extras:
      #layer_decay: 0.75
      layer_decay: 0.75
      #decay_type: inverse_normal
      decay_type: right
      
  network:
    pretrained_weights_path: /mnt/work/bird2vec/logs_pretrain_audioset_MAE/pretrain_xcl_wave_huge/runs/XCL/AudioMAE/2025-01-05_145126/callback_checkpoints/AudioMAE_XCL_epoch=99.ckpt
    ema_update_rate: null
    name: VIT_ppnet
    freeze_backbone: true
    ppnet:
      num_prototypes: 20
      last_layer_lr: 3e-4 